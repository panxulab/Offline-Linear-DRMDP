{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMDP_train():\n",
    "    def __init__(self, action_space, delta, xi_norm, seed=1):\n",
    "        np.random.seed(seed)\n",
    "        self.state_space = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "        self.action_space = action_space\n",
    "        self.initial_state = 'x1'\n",
    "        self.theta = [np.zeros(4),\n",
    "                      np.array([0,0,0,1]),\n",
    "                      np.array([0,0,0,1])]\n",
    "        self.delta = delta\n",
    "        self.H = 3\n",
    "        Xi = np.full(len(self.action_space[0]), 1)\n",
    "        self.Xi = xi_norm * Xi / np.linalg.norm(Xi, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.S = [self.initial_state]\n",
    "        self.A = [] # save the action history\n",
    "        self.R = [] # save the reward history\n",
    "        self.h = 0 # reset the step to 0\n",
    "        self.feature = [] # save the feature trajectory\n",
    "        self.feature_a = [] # save the full feature trajectory\n",
    "        self.current_state = self.initial_state # reset the current state to initial state\n",
    "\n",
    "    def phi(self, current_state, A):\n",
    "        if current_state == 'x1':\n",
    "            phi = np.array([1 - self.delta - self.Xi @ A, 0, 0, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x2':\n",
    "            phi = np.array([0, 1 - self.delta - self.Xi @ A, 0, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x3':\n",
    "            phi = np.array([0, 0, 1 - self.delta - self.Xi @ A, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x4':\n",
    "            phi = np.array([0, 0, 1, 0])\n",
    "        else:\n",
    "            phi = np.array([0, 0, 0, 1])\n",
    "        return phi\n",
    "\n",
    "    def add_state(self, s):\n",
    "        self.S.append(s)\n",
    "        self.current_state = s\n",
    "\n",
    "    def update_state(self, phi, h):\n",
    "        # calculate the transition probability\n",
    "        if h == 0:\n",
    "            prob = [phi[0] * (1 - 0.001), 0, phi[0] * 0.001, phi[3]]\n",
    "        elif h == 1:\n",
    "            if phi[2] == 1:\n",
    "                prob = [0, 0, 1, 0]\n",
    "            elif phi[3] == 1:\n",
    "                prob = [0, 0, 0, 1]\n",
    "            else:\n",
    "                prob = [0, phi[1] * (1 - 0.001), phi[1] * 0.001, phi[3]]\n",
    "        else:\n",
    "            prob = [0, 0, phi[2], phi[3]]\n",
    "        sprime = np.random.choice(range(1,5), size = 1, p = prob)[0]\n",
    "        return self.state_space[sprime] # return a string\n",
    "    \n",
    "    def next_state(self, phi):\n",
    "        next_state = self.update_state(phi, self.h)\n",
    "        self.add_state(next_state)\n",
    "        return next_state\n",
    "    \n",
    "    def generate_reward(self, phi):\n",
    "        reward = np.dot(phi, self.theta[self.h])\n",
    "        self.R.append(reward)\n",
    "        return reward\n",
    "    \n",
    "    def step(self, a):\n",
    "        self.A.append(a)\n",
    "        phi = self.phi(self.current_state, a)\n",
    "        phi_a = [self.phi(self.current_state, a) for a in self.action_space]\n",
    "        self.feature.append(phi)\n",
    "        self.feature_a.append(phi_a)\n",
    "        self.generate_reward(phi)\n",
    "        self.next_state(phi)\n",
    "        self.h += 1\n",
    "        \n",
    "\n",
    "class LinearMDP_test():\n",
    "    \"\"\"Perturbed evironment\"\"\"\n",
    "    def __init__(self,  nominal_MDP, q, seed=1):\n",
    "        np.random.seed(seed)\n",
    "        self.state_space = nominal_MDP.state_space\n",
    "        self.action_space = nominal_MDP.action_space\n",
    "        self.initial_state = 'x1'\n",
    "        self.theta = nominal_MDP.theta\n",
    "        self.delta = nominal_MDP.delta\n",
    "        self.Xi = nominal_MDP.Xi\n",
    "        self.H = 3\n",
    "        self.q = q\n",
    "\n",
    "    def reset(self):\n",
    "        self.S = [self.initial_state] # save the feature trajectory\n",
    "        self.A = [] # save the action history\n",
    "        self.R = [] # save the reward history\n",
    "        self.h = 0 # reset the step to 0\n",
    "        self.feature = [] # save the feature trajectory\n",
    "        self.feature_a = [] # save the full feature trajectory\n",
    "        self.current_state = self.initial_state # reset the current  state to initial state\n",
    "\n",
    "    def phi(self, current_state, A):\n",
    "        if current_state == 'x1':\n",
    "            phi = np.array([1 - self.delta - self.Xi @ A, 0, 0, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x2':\n",
    "            phi = np.array([0, 1 - self.delta - self.Xi @ A, 0, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x3':\n",
    "            phi = np.array([0, 0, 1 - self.delta - self.Xi @ A, self.delta + self.Xi @ A])\n",
    "        elif current_state == 'x4':\n",
    "            phi = np.array([0,0,1,0])\n",
    "        else:\n",
    "            phi = np.array([0,0,0,1])\n",
    "        return phi\n",
    "    \n",
    "    def add_state(self, s): \n",
    "        self.S.append(s)\n",
    "        self.current_state = s\n",
    "\n",
    "    def update_state(self, phi, h):\n",
    "        # calculate the transition probability  --- perturbed\n",
    "        if  h == 0:\n",
    "            prob = [phi[0], 0, self.q * phi[3], (1 - self.q) * phi[3]]\n",
    "        elif h == 1:\n",
    "            if phi[2] == 1:\n",
    "                prob = [0, 0, 1, 0]\n",
    "            elif phi[3] == 1:\n",
    "                prob = [0, 0, 0, 1]\n",
    "            else:\n",
    "                prob = [0, phi[1] * (1 - 0.001), phi[1] * 0.001, phi[3]]\n",
    "        else:\n",
    "            prob =  [0, 0, phi[2], phi[3]]\n",
    "        sprime = np.random.choice(range(1,5), size = 1, p = prob)[0]\n",
    "        return self.state_space[sprime] # return a string\n",
    "    \n",
    "    def next_state(self, phi):\n",
    "        next_state = self.update_state(phi, self.h)\n",
    "        self.add_state(next_state)\n",
    "        return next_state\n",
    "    \n",
    "    def generate_reward(self, phi):\n",
    "        reward = np.dot(phi, self.theta[self.h])\n",
    "        self.R.append(reward)\n",
    "        return reward\n",
    "\n",
    "    def step(self, a):\n",
    "        self.A.append(a)\n",
    "        phi = self.phi(self.current_state, a)\n",
    "        phi_a = [self.phi(self.current_state, a) for a in self.action_space]\n",
    "        self.feature.append(phi)\n",
    "        self.feature_a.append(phi_a)\n",
    "        self.generate_reward(phi)\n",
    "        self.next_state(phi)\n",
    "        self.h += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSVI_LCB():\n",
    "    def __init__(self, A, beta, H, lam, dataset, fail_state=None):\n",
    "        self.lam = lam\n",
    "        self.H = H\n",
    "        self.action_space = A\n",
    "        self.beta = beta\n",
    "        self.w = [np.zeros(4) for _ in range(self.H)]\n",
    "        self.fail_state = fail_state\n",
    "        self.Lambda = [self.lam * np.diag(np.ones(4)) for _ in range(self.H)]\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def get_action(self, phi_a, h):\n",
    "        Q_h = [self.get_Q_func(phi_a[idx], h) for idx in range (len(self.action_space))]\n",
    "        return self.action_space[np.argmax(Q_h)]\n",
    "    \n",
    "    def get_Q_func(self, phi, h, s_f=False):\n",
    "        if s_f == True:\n",
    "            return 0\n",
    "        else:\n",
    "            Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "            Q_h = np.min([(self.w[h] @ phi - self.beta * np.sqrt(phi @ Lambda_h_inverse @ phi)), self.H])\n",
    "            return Q_h\n",
    "    \n",
    "    def update_Q(self):\n",
    "        # Backward induction\n",
    "        self.w = [None for _ in range(self.H)] #initialize weights w\n",
    "        for h in range(self.H-1, -1, -1):\n",
    "            # calculate Lambda_h\n",
    "            for n in range(self.dataset['k']):\n",
    "                feature_temp = self.dataset['phi'][n][h]\n",
    "                self.Lambda[h] += np.outer(feature_temp, feature_temp)\n",
    "            #  calculate w_h\n",
    "            Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "            w_h = np.zeros(4)\n",
    "            if h == self.H - 1:\n",
    "                for tau in range(self.dataset['k']):\n",
    "                    phi_tau_h = self.dataset['phi'][tau][h]\n",
    "                    r_tau_h = self.dataset['r'][tau][h]\n",
    "                    w_h += Lambda_h_inverse @ (phi_tau_h * r_tau_h)\n",
    "            else:\n",
    "                for tau in range(self.dataset['k']):\n",
    "                    phi_tau_h = self.dataset['phi'][tau][h]\n",
    "                    phi_tau_h_plus_one = self.dataset['phi_a'][tau][h+1]\n",
    "                    r_tau_h = self.dataset['r'][tau][h]\n",
    "                    s_f_h_plus_one = (self.dataset['state'][tau][h+1] == self.fail_state)\n",
    "                    Q_tau_h_plus_one = [self.get_Q_func(phi_tau_h_plus_one[idx], h + 1, s_f_h_plus_one)\n",
    "                                    for idx in range(len(self.action_space))]\n",
    "                    V_tau_h_plus_one = np.max(Q_tau_h_plus_one)\n",
    "                    w_h += Lambda_h_inverse @ (phi_tau_h * (r_tau_h + V_tau_h_plus_one))\n",
    "            self.w[h] = w_h\n",
    "\n",
    "class DR_LSVI_LCB():\n",
    "    def __init__(self, A, beta, H, lam, dataset, Rho, theta, fail_state=None):\n",
    "        self.lam = lam\n",
    "        self.H = H\n",
    "        self.action_space = A\n",
    "        self.beta = beta\n",
    "        self.w = [np.zeros(4) for _ in range(self.H)]\n",
    "        self.Lambda = [self.lam * np.diag(np.ones(4)) for _ in range(self.H)]\n",
    "        self.Rho = Rho\n",
    "        self.theta = theta\n",
    "        self.fail_state = fail_state\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def get_action(self, phi_a, h):\n",
    "        Q_h = [self.get_Q_func(phi_a[idx], h) for idx in range(len(self.action_space))]\n",
    "        return self.action_space[np.argmax(Q_h)]\n",
    "\n",
    "    def get_Q_func(self, phi, h, s_f=False):\n",
    "        if s_f == True:\n",
    "            return 0\n",
    "        else:\n",
    "            Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "            penalty = self.beta * np.sqrt(phi @ np.diag(np.diagonal(Lambda_h_inverse)) @ phi)\n",
    "            Q_h = np.min([(self.w[h] @ phi - penalty), self.H - h])\n",
    "            return Q_h\n",
    "\n",
    "    def get_nu_h(self, h, rho):\n",
    "        Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "        nu_h = np.zeros(4)\n",
    "        Phi_h = np.zeros((0,4))\n",
    "        V_h_plus_one = np.zeros(0)\n",
    "        for tau in range(self.dataset['k']):\n",
    "            phi_tau_h = self.dataset['phi'][tau][h]\n",
    "            Phi_h = np.vstack((Phi_h, phi_tau_h))\n",
    "            phi_tau_h_plus_one = self.dataset['phi_a'][tau][h+1]\n",
    "            s_f_h_plus_one = (self.dataset['state'][tau][h+1] == self.fail_state)\n",
    "            Q_tau_h_plus_one = [self.get_Q_func(phi_tau_h_plus_one[idx], h+1, s_f_h_plus_one)\n",
    "                                for idx in range(len(self.action_space))]\n",
    "            V_tau_h_plus_one = np.max(Q_tau_h_plus_one)\n",
    "            V_h_plus_one = np.hstack((V_h_plus_one, V_tau_h_plus_one))\n",
    "        for i in range(4):\n",
    "            def z_alpha_i(alpha):\n",
    "                # compact formular for z\n",
    "                z = Lambda_h_inverse @ Phi_h.T @ np.minimum(V_h_plus_one, alpha)\n",
    "                return -z[i] + rho[i] * alpha\n",
    "            result = minimize(z_alpha_i, self.H/2, method='Nelder-Mead', bounds=[(0, self.H)])\n",
    "            nu_h[i] = - result.fun\n",
    "            return nu_h\n",
    "\n",
    "    def update_Q(self):\n",
    "        # Backward induction\n",
    "        self.w = [None for _ in range(self.H)] # innitialize weights w\n",
    "        for h in range(self.H-1, -1, -1):\n",
    "            # calculate Lambda_h\n",
    "            for n in range(self.dataset['k']):\n",
    "                feature_temp = self.dataset['phi'][n][h]\n",
    "                self.Lambda[h] += np.outer(feature_temp, feature_temp)\n",
    "            # update w_h\n",
    "            w_h = np.zeros(4)\n",
    "            nu_h = np.zeros(4)\n",
    "            if h == self.H - 1:\n",
    "                w_h = self.theta[h]\n",
    "            else:\n",
    "                nu_h = self.get_nu_h(h, rho=self.Rho[h])\n",
    "                w_h = self.theta[h] + nu_h\n",
    "            self.w[h] = w_h\n",
    "\n",
    "class VA_DR_LSVI_LCB():\n",
    "    def __init__(self, pre_agent, A, beta, H, lam, dataset, Rho, theta, fail_state=None):\n",
    "        self.lam = lam\n",
    "        self.H = H\n",
    "        self.action_space = A\n",
    "        self.beta = beta\n",
    "        self.w = [np.zeros(4) for _ in range(self.H)]\n",
    "        self.Lambda = [self.lam * np.diag(np.ones(4)) for _ in range(self.H)]\n",
    "        self.Rho = Rho\n",
    "        self.theta = theta\n",
    "        self.fail_state = fail_state\n",
    "        self.dataset = dataset\n",
    "        self.pre_agent = pre_agent\n",
    "        self.variance = {}\n",
    "\n",
    "    def get_variance_coefficient(self, h):\n",
    "        Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "        z1_h = np.zeros(4)\n",
    "        z2_h = np.zeros(4)\n",
    "        Phi_h = np.zeros((0,4))\n",
    "        V_h_plus_one = np.zeros(0)\n",
    "        for tau in range(self.dataset['k']):\n",
    "            phi_tau_h = self.dataset['phi'][tau][h]\n",
    "            Phi_h = np.vstack((Phi_h, phi_tau_h))\n",
    "            phi_tau_h_plus_one = self.dataset['phi_a'][tau][h+1]\n",
    "            s_f_h_plus_one = (self.dataset['state'][tau][h+1] == self.fail_state)\n",
    "            Q_tau_h_plus_one = [self.pre_agent.get_Q_func(phi_tau_h_plus_one[idx], h+1, s_f_h_plus_one)\n",
    "                                for idx in range(len(self.action_space))]\n",
    "            V_tau_h_plus_one = np.max(Q_tau_h_plus_one)\n",
    "            V_h_plus_one = np.hstack((V_h_plus_one, V_tau_h_plus_one))\n",
    "        \n",
    "        z1_h = Lambda_h_inverse @ Phi_h.T @ V_h_plus_one\n",
    "        z2_h = Lambda_h_inverse @ Phi_h.T @ V_h_plus_one**2\n",
    "        return z1_h, z2_h\n",
    "    \n",
    "    def estimated_variance(self, phi, z1_h, z2_h):\n",
    "        second_order_term = np.min([np.max([0, np.dot(phi, z2_h)]), self.H**2])\n",
    "        first_order_term = np.min([np.max([0, np.dot(phi, z1_h)]), self.H])\n",
    "        sigma_square = np.max([1, second_order_term - first_order_term**2])\n",
    "        return sigma_square\n",
    "    \n",
    "    def get_action(self, phi_a, h):\n",
    "        Q_h = [self.get_Q_func(phi_a[idx], h) for idx in range(len(self.action_space))]\n",
    "        return self.action_space[np.argmax(Q_h)]\n",
    "    \n",
    "    def get_Q_func(self, phi, h, s_f=False):\n",
    "        if s_f == True:\n",
    "            return 0\n",
    "        else:\n",
    "            Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "            penalty = self.beta * np.sqrt(phi @ np.diag(np.diagonal(Lambda_h_inverse)) @ phi)\n",
    "            Q_h = np.min([(self.w[h] @ phi - penalty), self.H - h])\n",
    "            return Q_h\n",
    "\n",
    "    def get_nu_h(self, h, rho, variance):\n",
    "        Lambda_h_inverse = np.linalg.inv(self.Lambda[h])\n",
    "        nu_h = np.zeros(4)\n",
    "        Phi_h = np.zeros((0, 4))\n",
    "        V_h_plus_one = np.zeros(0)\n",
    "        for tau in range(self.dataset['k']):\n",
    "            phi_tau_h = self.dataset['phi'][tau][h]\n",
    "            Phi_h = np.vstack((Phi_h, phi_tau_h))\n",
    "            phi_tau_h_plus_one = self.dataset['phi_a'][tau][h+1]\n",
    "            s_f_h_plus_one = (self.dataset['state'][tau][h+1] == self.fail_state)\n",
    "            Q_tau_h_plus_one = [self.get_Q_func(phi_tau_h_plus_one[idx], h+1, s_f_h_plus_one)\n",
    "                                for idx in range(len(self.action_space))]\n",
    "            V_tau_h_plus_one = np.max(Q_tau_h_plus_one)\n",
    "            V_h_plus_one = np.hstack((V_h_plus_one, V_tau_h_plus_one))\n",
    "        for i in range(4):\n",
    "            def z_alpha_i(alpha):\n",
    "                # compact formular for z\n",
    "                z = Lambda_h_inverse @ Phi_h.T @ (np.minimum(V_h_plus_one, alpha) / variance)\n",
    "                return -z[i] + rho[i] * alpha\n",
    "            result = minimize(z_alpha_i, self.H/2, method='Nelder-Mead', bounds=[(0, self.H)])\n",
    "            nu_h[i] = - result.fun\n",
    "            return nu_h\n",
    "        \n",
    "    def update_Q(self):\n",
    "        # Backward induction\n",
    "        self.w = [None for _ in range(self.H)] # initialize weights w\n",
    "        for h in range(self.H-1, -1, -1):\n",
    "            # calculate Lambda_h\n",
    "            if h == self.H-1:\n",
    "                for n in range(self.dataset['k']):\n",
    "                    feature_temp = self.dataset['phi'][n][h]\n",
    "                    self.Lambda[h] += np.outer(feature_temp, feature_temp)\n",
    "            else:\n",
    "                z1_h, z2_h = self.get_variance_coefficient(h)\n",
    "                self.variance[str(h)] = np.zeros(self.dataset['k'])\n",
    "                for n in range(self.dataset['k']):\n",
    "                    feature_temp = self.dataset['phi'][n][h]\n",
    "                    variance_temp = self.estimated_variance(feature_temp, z1_h, z2_h)\n",
    "                    self.variance[str(h)][n] = variance_temp\n",
    "                    self.Lambda[h] += np.outer(feature_temp, feature_temp) / variance_temp\n",
    "            # update w_h\n",
    "            w_h = np.zeros(4)\n",
    "            nu_h = np.zeros(4)\n",
    "            if h == self.H - 1:\n",
    "                w_h = self.theta[h]\n",
    "            else:\n",
    "                variance = self.variance[str(h)]\n",
    "                nu_h = self.get_nu_h(h, rho = self.Rho[h], variance = variance)\n",
    "                w_h = self.theta[h] + nu_h\n",
    "            self.w[h] = w_h\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Offline_Dataset_Collection(sample_size, env, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    history = {'k': 0, 'phi':[], 'r':[], 'state':[], 'phi_a':[]}    \n",
    "    epoch = sample_size\n",
    "    for t in range(epoch):\n",
    "        env.reset()\n",
    "        for h in range(env.H):\n",
    "            random_action_index = np.random.choice(range(0,len(env.action_space)), size = 1)[0]\n",
    "            action = env.action_space[random_action_index]\n",
    "            env.step(action)\n",
    "        # log the trajectory\n",
    "        history['phi_a'].append(env.feature_a)\n",
    "        history['phi'].append(env.feature)\n",
    "        history['r'].append(env.R)\n",
    "        history['state'].append(env.S)\n",
    "        history['k'] += 1\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once(dataset, action_space, beta, H, lam, fail_state):\n",
    "    agent = LSVI_LCB(A=action_space, beta=beta, H=H, lam=lam, dataset=dataset, fail_state=fail_state)\n",
    "    agent.update_Q()\n",
    "    return agent\n",
    "\n",
    "def train_once_DR(dataset, action_space, beta, H, lam, Rho, theta, fail_state):\n",
    "    agent = DR_LSVI_LCB(A=action_space, beta=beta, H=H, lam=lam, dataset=dataset, Rho=Rho, theta=theta, fail_state=fail_state)\n",
    "    agent.update_Q()\n",
    "    return agent\n",
    "\n",
    "def train_once_DR_VA(dataset, action_space, beta, H, lam, Rho, theta, pre_agent, fail_state):\n",
    "    agent = VA_DR_LSVI_LCB(pre_agent=pre_agent, A=action_space, beta=beta, H=H, lam=lam, dataset=dataset, Rho=Rho, theta=theta, fail_state=fail_state)\n",
    "    agent.update_Q()\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "probabilities are not non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-12fe7e33ce9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplication\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mOffline_Dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOffline_Dataset_Collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOffline_Dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfail_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfail_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mDR_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_once_DR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOffline_Dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfail_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfail_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-6747261a9f98>\u001b[0m in \u001b[0;36mOffline_Dataset_Collection\u001b[0;34m(sample_size, env, seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mrandom_action_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_action_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# log the trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phi_a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-bde371d243a5>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-bde371d243a5>\u001b[0m in \u001b[0;36mnext_state\u001b[0;34m(self, phi)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-bde371d243a5>\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, phi, h)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0msprime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msprime\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# return a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities are not non-negative"
     ]
    }
   ],
   "source": [
    "T1 = 100\n",
    "H = 3\n",
    "rho = 0.3\n",
    "beta = 1\n",
    "lam = 0.1\n",
    "actions = list(product([-1, 1], repeat=4))\n",
    "action_space = [np.array(action) for action in actions]\n",
    "delta = 0.3\n",
    "xi_norm = 0.1\n",
    "Rho = [[0,0,0,rho], [0,0,0,0]]\n",
    "fail_state = 'x4'\n",
    "replication = 20\n",
    "agent_dic = {}\n",
    "DR_agent_dic = {}\n",
    "VA_DR_agent_dic = {}\n",
    "env = LinearMDP_train(action_space, delta, xi_norm)\n",
    "\n",
    "for rep in range(replication):\n",
    "    Offline_Dataset = Offline_Dataset_Collection(T1, env, seed=rep)\n",
    "    agent = train_once(dataset=Offline_Dataset, action_space=action_space, beta=beta, H=H, lam=lam, fail_state=fail_state)\n",
    "    DR_agent = train_once_DR(dataset=Offline_Dataset, action_space=action_space, beta=beta, H=H, lam=lam, Rho=Rho, theta=env.theta, fail_state=fail_state)\n",
    "    VA_DR_agent = train_once_DR_VA(pre_agent=DR_agent, dataset=Offline_Dataset, action_space=action_space, beta=beta, H=H, lam=lam, Rho=Rho, theta=env.theta, fail_state=fail_state)\n",
    "    agent_dic[str(rep)] = agent\n",
    "    DR_agent_dic[str(rep)] = DR_agent\n",
    "    VA_DR_agent_dic[str(rep)] = VA_DR_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-6457583784ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0menv_test_DR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearMDP_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0menv_test_DR_VA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearMDP_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mDR_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDR_agent_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mVA_DR_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVA_DR_agent_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0'"
     ]
    }
   ],
   "source": [
    "Perturbation = [x / 20 for x in range(21)]\n",
    "T2 = 100\n",
    "R_LSVI_LCB = []\n",
    "R_DR_LSVI_LCB = []\n",
    "R_VA_DR_LSVI_LCB = []\n",
    "env = LinearMDP_train(action_space, delta, xi_norm)\n",
    "for q in Perturbation:\n",
    "    REWARD = 0\n",
    "    REWARD_DR = 0\n",
    "    REWARD_DR_VA = 0\n",
    "    for rep in range(replication):\n",
    "        reward = 0\n",
    "        reward_DR = 0\n",
    "        reward_DR_VA = 0\n",
    "        env_test = LinearMDP_test(env, q=q, seed=rep)\n",
    "        env_test_DR = LinearMDP_test(env, q=q, seed=rep)\n",
    "        env_test_DR_VA = LinearMDP_test(env, q=q, seed=rep)\n",
    "        agent = agent_dic[str(rep)]\n",
    "        DR_agent = DR_agent_dic[str(rep)]\n",
    "        VA_DR_agent = VA_DR_agent_dic[str(rep)]\n",
    "\n",
    "        for t in range(T2):\n",
    "            env_test.reset()\n",
    "            env_test_DR.reset()\n",
    "            env_test_DR_VA.reset()\n",
    "            for h in range(H):\n",
    "                # PEVI\n",
    "                current_state = env_test.current_state\n",
    "                phi_a = [env_test.phi(current_state, a) for a in action_space]\n",
    "                action = agent.get_action(phi_a, h)\n",
    "                env_test.step(action)\n",
    "                # DRPVI\n",
    "                current_state_DR = env_test_DR.current_state\n",
    "                phi_DR_a = [env_test_DR.phi(current_state_DR, a) for a in action_space]\n",
    "                action_DR = DR_agent.get_action(phi_DR_a, h)\n",
    "                env_test_DR.step(action_DR)\n",
    "                \n",
    "                # VA-DRPVI\n",
    "                current_state_DR_VA = env_test_DR_VA.current_state\n",
    "                phi_DR_VA_a = [env_test_DR_VA.phi(current_state_DR_VA, a) for a in action_space]\n",
    "                action_DR_VA = VA_DR_agent.get_action(phi_DR_VA_a, h)\n",
    "                env_test_DR_VA.step(action_DR_VA)\n",
    "                \n",
    "                \n",
    "\n",
    "            reward += np.sum(env_test.R) / T2\n",
    "            reward_DR += np.sum(env_test_DR.R) / T2   \n",
    "            reward_DR_VA += np.sum(env_test_DR_VA.R) / T2 \n",
    "\n",
    "        REWARD += reward / replication\n",
    "        REWARD_DR += reward_DR / replication \n",
    "        REWARD_DR_VA += reward_DR_VA / replication \n",
    "    \n",
    "    R_LSVI_LCB.append(REWARD)\n",
    "    R_DR_LSVI_LCB.append(REWARD_DR)\n",
    "    R_VA_DR_LSVI_LCB.append(REWARD_DR_VA)\n",
    "\n",
    "plt.plot(Perturbation, R_LSVI_LCB, label = 'PEVI')\n",
    "plt.plot(Perturbation, R_DR_LSVI_LCB, label = 'DRPVI')\n",
    "plt.plot(Perturbation, R_VA_DR_LSVI_LCB, label = 'VA-DRPVI')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('Perturbation', size=16)\n",
    "plt.ylabel('Average reward', size=16)\n",
    "plt.savefig(f'robustness_{delta}_{xi_norm}_{rho}.pdf', dpi=1000, bbox_inches='tight', pad_inches=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
